{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zlE3P6akcVZ1"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torchvision.transforms import Compose, Lambda, Normalize, RandomHorizontalFlip, RandomResizedCrop, ToTensor, RandomPerspective, Resize, RandomAdjustSharpness, RandomAutocontrast, RandomGrayscale, GaussianBlur\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    HfArgumentParser,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    ViTFeatureExtractor,\n",
        "    ViTMAEConfig,\n",
        "    ViTMAEForPreTraining,\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers.utils import check_min_version, send_example_telemetry\n",
        "from transformers.utils.versions import require_version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fvgZDoGVi7gS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Base model size so we can capture more in a vector\n",
        "'''\n",
        "configuration = ViTMAEConfig(\n",
        "    patch_size = 16,\n",
        "    image_size = 384,\n",
        "    mask_ratio = 0.6,\n",
        "    norm_pix_loss = True,\n",
        "    hidden_dropout_prob = 0.05,\n",
        "    attention_probs_dropout_prob = 0.05,\n",
        "    decoder_num_hidden_layers= 4,\n",
        "    decoder_num_attention_heads = 4,\n",
        "    decoder_hidden_size = 256,\n",
        "    decoder_intermediate_size = 1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YaytryGbc7Oh"
      },
      "outputs": [],
      "source": [
        "feature_extractor = ViTFeatureExtractor(\n",
        "    size = 384\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wO8024wmc86z"
      },
      "outputs": [],
      "source": [
        "model = ViTMAEForPreTraining(configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11QU-n9Mu4uX",
        "outputId": "82e6182f-0b05-4a48-b30f-a9efc3124595"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'height': 384, 'width': 384}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_extractor.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "e3Tjv1wbu2rj"
      },
      "outputs": [],
      "source": [
        "if \"shortest_edge\" in feature_extractor.size:\n",
        "  size = feature_extractor.size[\"shortest_edge\"]\n",
        "else:\n",
        "  size = (feature_extractor.size[\"height\"], feature_extractor.size[\"width\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A158A8ZVvKdP",
        "outputId": "afee974b-29b9-4ccb-a4b8-54d3e68c73d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(384, 384)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "51abf02cb87d4229a36c01d6866998ea",
            "6a0c7c5241d745e2b5304798c5c0f4a1",
            "4ab5555c14fa483b9a5274afa9fed01a",
            "721d602c6b694387bd1d04bd7ac02ebe",
            "2f9851d550284898a6ac1e6342bb9f09",
            "31c032fa22bf4604980128d4a5fdac78",
            "a204391cc76a4a14a185574912240f26",
            "9bb3c13d9794423683f780737ac52e21",
            "91a25778711b40c2a9d007929e5e3461",
            "cffbe1fbc48e4df78a44413f20b5b78b",
            "56494e8b465f4893ad9d13346df2853f"
          ]
        },
        "id": "8s6AY16Gf7Gd",
        "outputId": "9061579d-4a2a-4020-c5dc-74b76a424f66"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"cifar10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6IvOtVyBRSn",
        "outputId": "eb63a07f-86aa-4c3b-c743-0336807fde85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['train', 'test'])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcP0_ZgrZn2N",
        "outputId": "d83c380e-fddb-46cb-9898-100432f40492"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "img\n"
          ]
        }
      ],
      "source": [
        "column_names = ds[\"train\"].column_names\n",
        "image_column_name = column_names[0]\n",
        "print(image_column_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sR5lMh6wUUKx"
      },
      "outputs": [],
      "source": [
        "# transformations as done in original MAE paper\n",
        "# source: https://github.com/facebookresearch/mae/blob/main/main_pretrain.py\n",
        "transforms = Compose(\n",
        "    [\n",
        "        Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img),\n",
        "        # Resize(feature_extractor.size, interpolation=InterpolationMode.BICUBIC),\n",
        "        RandomPerspective(distortion_scale=0.45, p=0.15),\n",
        "        GaussianBlur(kernel_size=(5, 9), sigma=(0.01,2.0)),\n",
        "        RandomAdjustSharpness(sharpness_factor=0.35, p=0.25),\n",
        "        ToTensor(),\n",
        "        Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def preprocess_images(examples):\n",
        "    \"\"\"Preprocess a batch of images by applying transforms.\"\"\"\n",
        "    examples[\"pixel_values\"] = [transforms(image) for image in examples[image_column_name]]\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "536_Jq10YwoD",
        "outputId": "269cdfba-1c00-4a6c-96d4-deb4ea6430ec"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDeXxDYm8MHmfKB/rD93PpUsGt2UyufPRdpbAJwSB3rk544YA5cHkhAzLx/9aoAGXAQrn+EOM5FXcDsI9cs3BZpUVd20HPt1NQrrlrcX32SLc+QT5gHy1y5ljcZllUndt35rPeWaF2iDM6Y52jGc0XAt67ezWz+W6J5EjbhID3HbHrQlrqLW8c8MRkikHytCpf88dKimEVyNsyRsByDvIOfWizkuNKlkk0+/miaT7yvIWVj9DSvGw7GrD4c1CWMyfOpbBIaIjmpJPDN5wZZiqY67OlUpdb1W5t2iu79GyB9wbefzqODWb60iWJb1mj7rntU3QWP/9k=",
            "image/png": "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",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds['train']['img'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "-ao9Ra9EA-6o",
        "outputId": "7c4ff5f0-af87-4e43-e2a2-8408e94546f8"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwCjb3MU8kkSsS8Rw4IxjIqW4mhtrQyDzXmUktGqggL2Oc9c1pajDFAsn7sRzT3c21lHXbkDP4AflVmRItX0OW9NtFaK0aRnYu1XkA5H5H9K8n6mlVcW9D0FiG4X6nJrLc3lrJcKRCqPt2kbufeqsV3580ltJt81RnK9DWrp9lFbaTq0rnzH81Y0BOAF2/4msCS0kcw3cHySIcDn73H8q6p4enyWStY541p82rPTNSQavZ20lmm95p45IwnOC3UH8zmsjUdKXSNdksNT1G2htlIfYsxOckngY4zkcHpisbw7r58O65HPIhltUkLNEOxxgMPzrFvtSGo3txczzN50jlmZ+cn6168sJ71nseasV7t1uWNQv5YVa1sHD/eXzlchSCe49cY5pdPkhtLMeYGkm7knOPpVVYdkYmIDqfQ5q5NvsLaO4mjjcyH5EP8ACPXHeqeApTjysj67Ui7o/9k=",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJmklEQVR4ARVW2Y9bVx0+692v7evds3gymWyUUNLQBqhaFgmkUl544IFH/gT+HiQqHlFZqkq0QggiFUFS0iZtCs0kE5PJNDO2x/ty93sWfrau5CMv5577/b4Nv/OrV7BWBmeYkDzPhCwMw5BKaaUxkYQiXbgYSW6kFMFvtFSiEEopjDATEmcKY4Tg1xjjPC+kZLAbQTJXKhIoziXLEdE6QUqZyCWIMiYJQUgjzEmW50JRpgmFvQnCqkAigz8rRXNsSWrmsJAEK4mVsDjZnJFpWRQIC40k7EEpfKAE0pmWAkuqipzaBCMFOyolDc6F5qqgsBZwWq2JJpgamlqJNIfTIsp1GBZUS9+iBlYlx7ZNoUgOh6GUcoQKpeHEGaKaqMKkAjBAhBBK4AmE0ohgbtjtC1dWi8lkGnNmEGTmgiXaPjyZaLNaUDf3rHA5OxstPJPJ4aLbMmq+aTEAShiArJYMIYxZBRAUgB0RucgNakoptZLwjcHJt3/04/t37vYX00gA6O7J6fj47MysdHZa+9r0c2ZyryHScDrqO5XqaXieKtXyucOpLGKiEcuIv4wdKbLAEyUqmQbMcqwRvMGjxPH89p/fP19k5yE5OZufDF5Qy5O05Jbq3PGYZZuYWMSd5Elnp5sm0fHx+WyZUuxdaHhcKiwFGSd0mFT++NHjfx0NUsI40ElrGI5hcBgGEOn45Ph0ONVGQL1tEmzZnV2jVssB8cBtwy5ilcz7vqEqrpGnCfeb44i8OF+nADxmQC/Cyvsx9gqjMYv9OLek1lIDDQUhZiFLw8g7W0vsVYPWhUqtVa83fa/i+9U8K9JwFbiWZzCZJ1rky9kUKZlEETWc0UoMlqlkwCFErr58i9m+V27c+u5bjr+VC6woV4abk8BvXj+bWNzd3d677nkNzi2VFckq0nJzui8ffjE4PXVc13W86XQ2Xywxhv/YJmPzsDgeLgtqYcNgTrm2d/FKUqDu/qV6oRfHJ4UWUji3vvez7sVX97/x/P5nDwOv3R9NmDZMzoFgYRQt57PA5RqBKHS90cgKMZkvMSW+5zLK8jR+9uK0UbEv7/iEml7/fHzjW6+55Ro1YdoaHuzkxdoI9pGz47tNi3m24ViGCQhsb3XSNAapr9brclC7cu2lUqncbLUxoXBVgiqjG3HZTgUb1d6L9ekoItwqpWmeZQU3HMctuZZd4sxj8re//s2Xj47Gk6FhgjTE/sVt2yVSJO1mnbGNyC9eunRw6QrlPEnTVRQLqZIkrVTKpmWVKrVS0KR2cDqYgDJ5HEZpnHBuriNA1+ZIdSp0ctbrn/ZOzo6OT59gLrf32lugIoNWK5W9btfz/M7W9mK1KqQ6H0/BtzBlcZImSQLW5HputV4NahVQ04ZJVKtOveZY5u0v/hcIdbnKLVMaLB2Pnqts3j3Yp5bplIJ6a2c6C5erWErUaDQYN1OQZSGSFCxSwpVmuRCkVm9icACcmlhI7TDOaNmzK74NhrXS7mSO6z5zDS5J8bz/vBWU9y69lBbo3v3Ds8Hc9wLg0pe9rxAiCgFQIoySSrUqNB6cj1y/zKh2HMeAgRVTGS1aTRgyxu1mG/iq0qyzsz/OnQXeCmmzXK+WS7Cbf+HSS69+582zs1Ecx+ej0WA45Ay1A57OTqLFsFxyZ5Px+XCwWi3Bxx3Lprrg+YzG/bZb1GzM4G6loC0kM5l5Zb/76X1/xS8pvG5t80eHH7/+/V/evfNxFK2KfDIavoCDhwWopwjIfNteLcdPBQ1azQAQggmnSRxxsNOwSM+aPNnynEwkDAYS1OsCs5QYllcCGnz1YvjGa19PQ+X448HZae/oSMgckidaLf1aZ7mMy5519cr1Tx4+fvD4+Rs/+AnQ71mvt1zHAFqahHst33btatXXTIgcfFrE5apHLR5DQhDS3d2J03wZK+52dw9uDvqDw8PH9VoNdLC9tX1h/0BjnmSg9GqpsfvKa2+Mx9M7d+5GcbJYhqZhlvVgz5te7ajAWjE9dXFK1tOBzQuGU6zgEvVqDRE6mkUnw4hY7WvXXwaVFhItVnElaF3eP9jb6kzHk+lkzk0vqG/N1ulwugpTRS0fRnjQrHd9uwLJlikmOIQZe9Z71r38NYvkKk+YtXn5vueVSteuXf3bXz+Ml0On2uydjnZ3uvtXb5oGu9jtLmbzR4dPlZZni3yVyFSaq0XcbO98NY2ru+WpCZrPF0BRZmUqZ5/3Rt3rtxSKsBCgCfCAxWJSq954+60f3vjmtXf/9B5IqFwOtrd2YEBURNU26+wXS9v67OHDQYg1L5XbtfpBmTJwYvxEu72hNCiGkccCYpGyo6U9kb7mKcmXWlEIzK1O883Xb1pc7u9t//Tnv/jDex9MhsvBUqVpz0BilojeyRDlha5fDZqOQiBirixHYaOQeim5xQ2L4QjHBedaFexoQd7/539u7NXbhutw1mm3O/XSwcUdpPPBePrO7z548PmjDCQroGpAacmlWZIElGALTAWxLchcjdN8UwcYs6Af6HQDBVcb/8sLTEJi/P3B0e9v33s6DkOpj5893W0FFudhzt79yyefPerHwpSsROwKsnzilYkJ5UZmmKQQ3LLIBEoF1A2oNgQkXLG5zTk2XMmdQmPDr7BavTGb68F8cefhY1nsIWQ02juYmvc+/e8Ht+9mykFsY6cIrD/LoY1BhdGb1MMcqgPUG2owqCiUATUAX6IBJ3ARDkHRbpf9UpnB1+CjIjWen6+y6PB7N6/Ylc4yVR/9+9NUg/gL07SgyIFPwD0gyDZFTiOTQssCf2HYhDJkM8aKQqyjCPInE6oMxtipexZL1mumhARwgcY5oqMwe/Ck/3as13p9Nl+bnidimmaZ49hgirCAVNn4O4O2x0CW3LTCQuYignvAY8HWUZp7lXql0QafffL4MVeSADWhosBjKGJJ7j0frd9598PDk/Pj/jjKCiAJtwxqGI4PLC1DU4KTZoCVRoALrCmYJdJJHMZRCAsItVa7M5nOer3eydETBF0VAiRN11ECfcsW0Pu4+Y97Xxz3+8uomIWJyJHretAyTNMEX7RsSQll3JCIQPXDajMOKKN5kduWBY4S1DtgP5nBEtNQjEdpwrI0AV5ksuDUENClYVC2d9IfE0YFxL8A+qdRFMHHcA/ICdu2CFGGZdqOB+V4MpspJBgnQcltVSvtdnURZevFPFwuICcm4wnLktSk2IFkKxIMnRdaLzAFUTBCvem7mxcMGW4wn89nRVLy3HJQLVFiIUuqjEFnNmmWZiYDAkgRL0WchYsp9GjL5Cml/wdMhZEncF1iAgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds['test']['img'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_WRWux79d_rx"
      },
      "outputs": [],
      "source": [
        "ds[\"train\"].set_transform(preprocess_images)\n",
        "ds[\"test\"].set_transform(preprocess_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oA8npwZdQkwp",
        "outputId": "6af2ae86-3801-4ab1-8959-a3e0f6f7c57d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nhttps://github.com/huggingface/transformers/issues/19802\\nYou'll need to add remove_unused_columns=False to the TrainingArguments.\\n\\nThe reason is because of the use of set_transform when preparing the datasets, which does things on-the-fly.\\nHence we still need to image column in the datasets to turn them into pixel_values.\\n\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "https://github.com/huggingface/transformers/issues/19802\n",
        "You'll need to add remove_unused_columns=False to the TrainingArguments.\n",
        "\n",
        "The reason is because of the use of set_transform when preparing the datasets, which does things on-the-fly.\n",
        "Hence we still need to image column in the datasets to turn them into pixel_values.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XcE9CUHSSaEg"
      },
      "outputs": [],
      "source": [
        "def get_training_args(output_dir: str, overwrite_output_dir : bool = True, batch_size : int = 128, num_epochs: int = 10):\n",
        "    return TrainingArguments(\n",
        "        output_dir = output_dir,\n",
        "        overwrite_output_dir = overwrite_output_dir,\n",
        "        per_device_train_batch_size = batch_size,\n",
        "        num_train_epochs = num_epochs,\n",
        "        learning_rate=1.5e-4,\n",
        "        warmup_ratio = 0.05,\n",
        "        logging_strategy = 'epoch',\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit = 100,\n",
        "        run_name=\"vit-mae-test\",\n",
        "        remove_unused_columns=False,\n",
        "        tf32=True,\n",
        "        dataloader_num_workers = os.cpu_count(),\n",
        "        optim=\"adamw_torch\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        weight_decay=0.05,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nSHcbfJcS8-x"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(output_dir = \"./test\",\n",
        "             overwrite_output_dir = True,\n",
        "             per_device_train_batch_size = 8,\n",
        "             num_train_epochs = 100,\n",
        "             learning_rate=1.5e-4,\n",
        "             warmup_ratio = 0.05,\n",
        "             logging_strategy = 'epoch',\n",
        "              evaluation_strategy='epoch',\n",
        "              save_strategy='epoch',\n",
        "              load_best_model_at_end=True,\n",
        "              save_total_limit = 100,\n",
        "              run_name=\"vit-mae-test\",\n",
        "              remove_unused_columns=False,\n",
        "              tf32=False,\n",
        "              dataloader_num_workers = os.cpu_count(),\n",
        "              optim=\"adamw_torch\",\n",
        "              lr_scheduler_type=\"cosine\",\n",
        "              weight_decay=0.05,\n",
        "          )\n",
        "\n",
        "# Compute absolute learning rate\n",
        "total_train_batch_size = (\n",
        "    training_args.train_batch_size * training_args.gradient_accumulation_steps * training_args.world_size\n",
        ")\n",
        "training_args.learning_rate = training_args.learning_rate * total_train_batch_size / 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9NZM2SPZslm",
        "outputId": "22854ab7-8c19-44c7-a64f-0c13d56584c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args.world_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4PDQjE19Tj_q"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DAEx0KLIdfM_"
      },
      "outputs": [],
      "source": [
        "# Initialize our trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds[\"train\"],\n",
        "    eval_dataset=ds[\"test\"],\n",
        "    tokenizer=feature_extractor,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M_HdSLgw4Z4"
      },
      "outputs": [],
      "source": [
        "#more gradient updates seem to work - gradient accumulation might be hurting\n",
        "# 24 layers isn't better all the time\n",
        "# 24 attention heads is worse than 6 attention heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IBtYhHNVAAYo",
        "outputId": "8bee9fb8-bb09-4e5b-9738-62ebc872b2df"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e369e4d485b74fe2ae3b3b3a0cdedbe3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/625000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'preprocess_images' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'preprocess_images' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'preprocess_images' on <module '__main__' (built-in)>\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'preprocess_images' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'preprocess_images' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'preprocess_images' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'preprocess_images' on <module '__main__' (built-in)>\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 120, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/lenn/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/spawn.py\", line 130, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: Can't get attribute 'preprocess_images' on <module '__main__' (built-in)>\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "DataLoader worker (pid(s) 80078) exited unexpectedly",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    424\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    931\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 80078) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/Users/lenn/Library/CloudStorage/GoogleDrive-lennard.ong@gmail.com/My Drive/_XLX_OurSharedFolder/Private - Lenn/02_NUS_CS5242NeuralNets1/NUS-NeuralNetworks/project/ViTMAE pretrain test.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lenn/Library/CloudStorage/GoogleDrive-lennard.ong%40gmail.com/My%20Drive/_XLX_OurSharedFolder/Private%20-%20Lenn/02_NUS_CS5242NeuralNets1/NUS-NeuralNetworks/project/ViTMAE%20pretrain%20test.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#75% masking 12 layers, 12 attention heads, 16 patch size, decoder with 4 attention heads and 2 layers 192 batch size\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lenn/Library/CloudStorage/GoogleDrive-lennard.ong%40gmail.com/My%20Drive/_XLX_OurSharedFolder/Private%20-%20Lenn/02_NUS_CS5242NeuralNets1/NUS-NeuralNetworks/project/ViTMAE%20pretrain%20test.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/site-packages/transformers/trainer.py:1599\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1598\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1599\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1600\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1601\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1602\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1603\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1604\u001b[0m     )\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/site-packages/transformers/trainer.py:1878\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1875\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1879\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1880\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/site-packages/accelerate/data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[1;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[39myield\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1296\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/neuralnet311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1145\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1146\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[1;32m   1148\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 80078) exited unexpectedly"
          ]
        }
      ],
      "source": [
        "#75% masking 12 layers, 12 attention heads, 16 patch size, decoder with 4 attention heads and 2 layers 192 batch size\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z_rnSqxKR6ak",
        "outputId": "70150cbe-ee0c-4252-911d-63e700c797d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 384\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 384\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1310\n",
            "  Number of trainable parameters = 88605888\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1310' max='1310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1310/1310 40:39, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.806000</td>\n",
              "      <td>0.662734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.613300</td>\n",
              "      <td>0.575301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.567500</td>\n",
              "      <td>0.563053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.549400</td>\n",
              "      <td>0.532238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.521400</td>\n",
              "      <td>0.505097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.501300</td>\n",
              "      <td>0.486622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.483900</td>\n",
              "      <td>0.468811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.473500</td>\n",
              "      <td>0.460322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.467500</td>\n",
              "      <td>0.455817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.465500</td>\n",
              "      <td>0.454984</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-131\n",
            "Configuration saved in test-vit/checkpoint-131/config.json\n",
            "Model weights saved in test-vit/checkpoint-131/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-131/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-262\n",
            "Configuration saved in test-vit/checkpoint-262/config.json\n",
            "Model weights saved in test-vit/checkpoint-262/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-262/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-393\n",
            "Configuration saved in test-vit/checkpoint-393/config.json\n",
            "Model weights saved in test-vit/checkpoint-393/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-393/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-524\n",
            "Configuration saved in test-vit/checkpoint-524/config.json\n",
            "Model weights saved in test-vit/checkpoint-524/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-524/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-655\n",
            "Configuration saved in test-vit/checkpoint-655/config.json\n",
            "Model weights saved in test-vit/checkpoint-655/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-655/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-786\n",
            "Configuration saved in test-vit/checkpoint-786/config.json\n",
            "Model weights saved in test-vit/checkpoint-786/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-786/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-917\n",
            "Configuration saved in test-vit/checkpoint-917/config.json\n",
            "Model weights saved in test-vit/checkpoint-917/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-917/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1048\n",
            "Configuration saved in test-vit/checkpoint-1048/config.json\n",
            "Model weights saved in test-vit/checkpoint-1048/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1048/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1179\n",
            "Configuration saved in test-vit/checkpoint-1179/config.json\n",
            "Model weights saved in test-vit/checkpoint-1179/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1179/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1310\n",
            "Configuration saved in test-vit/checkpoint-1310/config.json\n",
            "Model weights saved in test-vit/checkpoint-1310/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1310/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-1310 (score: 0.45498406887054443).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1310, training_loss=0.5449381296871273, metrics={'train_runtime': 2472.0194, 'train_samples_per_second': 202.264, 'train_steps_per_second': 0.53, 'total_flos': 1.17938479693824e+20, 'train_loss': 0.5449381296871273, 'epoch': 10.0})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#75% masking 12 layers, 12 attention heads, 24 patch size, decoder with 4 attention heads and 2 layers 384 batch size\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9Sx_O1QyufYA",
        "outputId": "1f21f078-6288-40ab-98dc-d13c5f7d3ca2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 192\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2610\n",
            "  Number of trainable parameters = 87621888\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2610' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2610/2610 27:02, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.721300</td>\n",
              "      <td>0.574548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.510900</td>\n",
              "      <td>0.466157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.462900</td>\n",
              "      <td>0.451724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.441900</td>\n",
              "      <td>0.421916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.403900</td>\n",
              "      <td>0.382665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.371600</td>\n",
              "      <td>0.343169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.345000</td>\n",
              "      <td>0.321825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.332600</td>\n",
              "      <td>0.312614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.326500</td>\n",
              "      <td>0.308952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.324700</td>\n",
              "      <td>0.308087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-261\n",
            "Configuration saved in test-vit/checkpoint-261/config.json\n",
            "Model weights saved in test-vit/checkpoint-261/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-261/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-522\n",
            "Configuration saved in test-vit/checkpoint-522/config.json\n",
            "Model weights saved in test-vit/checkpoint-522/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-522/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-783\n",
            "Configuration saved in test-vit/checkpoint-783/config.json\n",
            "Model weights saved in test-vit/checkpoint-783/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-783/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1044\n",
            "Configuration saved in test-vit/checkpoint-1044/config.json\n",
            "Model weights saved in test-vit/checkpoint-1044/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1044/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1305\n",
            "Configuration saved in test-vit/checkpoint-1305/config.json\n",
            "Model weights saved in test-vit/checkpoint-1305/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1305/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1566\n",
            "Configuration saved in test-vit/checkpoint-1566/config.json\n",
            "Model weights saved in test-vit/checkpoint-1566/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1566/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1827\n",
            "Configuration saved in test-vit/checkpoint-1827/config.json\n",
            "Model weights saved in test-vit/checkpoint-1827/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1827/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2088\n",
            "Configuration saved in test-vit/checkpoint-2088/config.json\n",
            "Model weights saved in test-vit/checkpoint-2088/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2088/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2349\n",
            "Configuration saved in test-vit/checkpoint-2349/config.json\n",
            "Model weights saved in test-vit/checkpoint-2349/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2349/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2610\n",
            "Configuration saved in test-vit/checkpoint-2610/config.json\n",
            "Model weights saved in test-vit/checkpoint-2610/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2610/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-2610 (score: 0.30808666348457336).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2610, training_loss=0.42411637287943754, metrics={'train_runtime': 1638.4045, 'train_samples_per_second': 305.175, 'train_steps_per_second': 1.593, 'total_flos': 1.17067474796544e+20, 'train_loss': 0.42411637287943754, 'epoch': 10.0})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#75% masking 12 layers, 12 attention heads, decoder with 4 attention heads and 2 layers 192 batch size\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "YGXzU3BlHyCj",
        "outputId": "85f25f3b-5c41-4d2b-8f11-0776416a635b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 640\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 640\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 790\n",
            "  Number of trainable parameters = 24081600\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='80' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 80/790 01:30 < 13:39, 0.87 it/s, Epoch 1/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.824300</td>\n",
              "      <td>0.718923</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-79\n",
            "Configuration saved in test-vit/checkpoint-79/config.json\n",
            "Model weights saved in test-vit/checkpoint-79/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-79/preprocessor_config.json\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2dfc390c1792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#75% masking 12 layers, 6 attention heads, 24 patch, decoder with 4 attention heads and 2 layers - can use 768 batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#75% masking 12 layers, 6 attention heads, 24 patch, decoder with 4 attention heads and 2 layers - can use 640 batch size\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FpnTmmeylOps",
        "outputId": "07dc83ad-f804-4dbe-8f8a-2d24830c2e5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 256\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1960\n",
            "  Number of trainable parameters = 23466240\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1960' max='1960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1960/1960 20:07, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.734300</td>\n",
              "      <td>0.553461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.503800</td>\n",
              "      <td>0.471630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.465500</td>\n",
              "      <td>0.452548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.431800</td>\n",
              "      <td>0.405063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.391400</td>\n",
              "      <td>0.362184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.356400</td>\n",
              "      <td>0.332269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.336800</td>\n",
              "      <td>0.315196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.325500</td>\n",
              "      <td>0.306295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.319800</td>\n",
              "      <td>0.302081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.318300</td>\n",
              "      <td>0.301795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-196\n",
            "Configuration saved in test-vit/checkpoint-196/config.json\n",
            "Model weights saved in test-vit/checkpoint-196/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-196/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-392\n",
            "Configuration saved in test-vit/checkpoint-392/config.json\n",
            "Model weights saved in test-vit/checkpoint-392/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-392/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-588\n",
            "Configuration saved in test-vit/checkpoint-588/config.json\n",
            "Model weights saved in test-vit/checkpoint-588/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-588/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-784\n",
            "Configuration saved in test-vit/checkpoint-784/config.json\n",
            "Model weights saved in test-vit/checkpoint-784/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-784/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-980\n",
            "Configuration saved in test-vit/checkpoint-980/config.json\n",
            "Model weights saved in test-vit/checkpoint-980/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-980/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1176\n",
            "Configuration saved in test-vit/checkpoint-1176/config.json\n",
            "Model weights saved in test-vit/checkpoint-1176/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1176/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1372\n",
            "Configuration saved in test-vit/checkpoint-1372/config.json\n",
            "Model weights saved in test-vit/checkpoint-1372/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1372/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1568\n",
            "Configuration saved in test-vit/checkpoint-1568/config.json\n",
            "Model weights saved in test-vit/checkpoint-1568/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1568/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1764\n",
            "Configuration saved in test-vit/checkpoint-1764/config.json\n",
            "Model weights saved in test-vit/checkpoint-1764/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1764/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1960\n",
            "Configuration saved in test-vit/checkpoint-1960/config.json\n",
            "Model weights saved in test-vit/checkpoint-1960/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1960/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-1960 (score: 0.3017953038215637).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1960, training_loss=0.41836337070075835, metrics={'train_runtime': 1224.2545, 'train_samples_per_second': 408.412, 'train_steps_per_second': 1.601, 'total_flos': 3.163221393408e+19, 'train_loss': 0.41836337070075835, 'epoch': 10.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#75% masking 12 layers, 6 attention heads, decoder with 4 attention heads and 2 layers - can use 256 batch size\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fSQeXhBrzFRM",
        "outputId": "d5e5635a-d8dd-45ef-9cc4-ae760a61322b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 192\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2610\n",
            "  Number of trainable parameters = 23466240\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2610' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2610/2610 23:27, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.721600</td>\n",
              "      <td>0.529469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.488200</td>\n",
              "      <td>0.455943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.452100</td>\n",
              "      <td>0.434037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.400200</td>\n",
              "      <td>0.370430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.363800</td>\n",
              "      <td>0.339253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.336700</td>\n",
              "      <td>0.308669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.314500</td>\n",
              "      <td>0.290110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.301700</td>\n",
              "      <td>0.279971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.295000</td>\n",
              "      <td>0.275347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.293100</td>\n",
              "      <td>0.274801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-261\n",
            "Configuration saved in test-vit/checkpoint-261/config.json\n",
            "Model weights saved in test-vit/checkpoint-261/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-261/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-522\n",
            "Configuration saved in test-vit/checkpoint-522/config.json\n",
            "Model weights saved in test-vit/checkpoint-522/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-522/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-783\n",
            "Configuration saved in test-vit/checkpoint-783/config.json\n",
            "Model weights saved in test-vit/checkpoint-783/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-783/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1044\n",
            "Configuration saved in test-vit/checkpoint-1044/config.json\n",
            "Model weights saved in test-vit/checkpoint-1044/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1044/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1305\n",
            "Configuration saved in test-vit/checkpoint-1305/config.json\n",
            "Model weights saved in test-vit/checkpoint-1305/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1305/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1566\n",
            "Configuration saved in test-vit/checkpoint-1566/config.json\n",
            "Model weights saved in test-vit/checkpoint-1566/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1566/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1827\n",
            "Configuration saved in test-vit/checkpoint-1827/config.json\n",
            "Model weights saved in test-vit/checkpoint-1827/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1827/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2088\n",
            "Configuration saved in test-vit/checkpoint-2088/config.json\n",
            "Model weights saved in test-vit/checkpoint-2088/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2088/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2349\n",
            "Configuration saved in test-vit/checkpoint-2349/config.json\n",
            "Model weights saved in test-vit/checkpoint-2349/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2349/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2610\n",
            "Configuration saved in test-vit/checkpoint-2610/config.json\n",
            "Model weights saved in test-vit/checkpoint-2610/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2610/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-2610 (score: 0.2748011648654938).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2610, training_loss=0.3966947504387048, metrics={'train_runtime': 1422.2928, 'train_samples_per_second': 351.545, 'train_steps_per_second': 1.835, 'total_flos': 3.163221393408e+19, 'train_loss': 0.3966947504387048, 'epoch': 10.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#12 layers, 6 attention heads, decoder with 4 attention heads and 2 layers - can use 192 batch size\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K7hik_er-wDI",
        "outputId": "c935621e-5017-4945-9fdc-add02b247978"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3910\n",
            "  Number of trainable parameters = 44759808\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3910' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3910/3910 36:58, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.699700</td>\n",
              "      <td>0.522085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.482000</td>\n",
              "      <td>0.454478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.433400</td>\n",
              "      <td>0.396622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.378300</td>\n",
              "      <td>0.342505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.333100</td>\n",
              "      <td>0.300412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.304600</td>\n",
              "      <td>0.280157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.289300</td>\n",
              "      <td>0.265880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.280600</td>\n",
              "      <td>0.259355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.276200</td>\n",
              "      <td>0.255832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.274300</td>\n",
              "      <td>0.255388</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-391\n",
            "Configuration saved in test-vit/checkpoint-391/config.json\n",
            "Model weights saved in test-vit/checkpoint-391/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-391/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-782\n",
            "Configuration saved in test-vit/checkpoint-782/config.json\n",
            "Model weights saved in test-vit/checkpoint-782/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-782/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1173\n",
            "Configuration saved in test-vit/checkpoint-1173/config.json\n",
            "Model weights saved in test-vit/checkpoint-1173/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1173/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1564\n",
            "Configuration saved in test-vit/checkpoint-1564/config.json\n",
            "Model weights saved in test-vit/checkpoint-1564/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1564/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1955\n",
            "Configuration saved in test-vit/checkpoint-1955/config.json\n",
            "Model weights saved in test-vit/checkpoint-1955/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1955/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2346\n",
            "Configuration saved in test-vit/checkpoint-2346/config.json\n",
            "Model weights saved in test-vit/checkpoint-2346/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2346/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2737\n",
            "Configuration saved in test-vit/checkpoint-2737/config.json\n",
            "Model weights saved in test-vit/checkpoint-2737/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2737/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3128\n",
            "Configuration saved in test-vit/checkpoint-3128/config.json\n",
            "Model weights saved in test-vit/checkpoint-3128/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3128/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3519\n",
            "Configuration saved in test-vit/checkpoint-3519/config.json\n",
            "Model weights saved in test-vit/checkpoint-3519/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3519/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 10000\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3910\n",
            "Configuration saved in test-vit/checkpoint-3910/config.json\n",
            "Model weights saved in test-vit/checkpoint-3910/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3910/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-3910 (score: 0.25538820028305054).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3910, training_loss=0.37514540143025193, metrics={'train_runtime': 2232.869, 'train_samples_per_second': 223.927, 'train_steps_per_second': 1.751, 'total_flos': 5.9890993201152e+19, 'train_loss': 0.37514540143025193, 'epoch': 10.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 24 layers encoder - 128 batch size\n",
        "# test - decoder with 4 attention heads if you check the config the decoder hiddne size, intermediate size is the same rate as encoder per head!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8Bl9xLzpwyTN",
        "outputId": "138f82ec-a236-4320-c7d8-92c452c020f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3910\n",
            "  Number of trainable parameters = 43996032\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3910' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3910/3910 33:40, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.723900</td>\n",
              "      <td>0.592738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.522200</td>\n",
              "      <td>0.479779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.467000</td>\n",
              "      <td>0.453773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.441900</td>\n",
              "      <td>0.409960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.401400</td>\n",
              "      <td>0.382229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.380800</td>\n",
              "      <td>0.362736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.368800</td>\n",
              "      <td>0.352512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.357600</td>\n",
              "      <td>0.339509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.348400</td>\n",
              "      <td>0.332841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.345700</td>\n",
              "      <td>0.332397</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-391\n",
            "Configuration saved in test-vit/checkpoint-391/config.json\n",
            "Model weights saved in test-vit/checkpoint-391/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-391/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-782\n",
            "Configuration saved in test-vit/checkpoint-782/config.json\n",
            "Model weights saved in test-vit/checkpoint-782/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-782/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1173\n",
            "Configuration saved in test-vit/checkpoint-1173/config.json\n",
            "Model weights saved in test-vit/checkpoint-1173/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1173/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1564\n",
            "Configuration saved in test-vit/checkpoint-1564/config.json\n",
            "Model weights saved in test-vit/checkpoint-1564/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1564/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1955\n",
            "Configuration saved in test-vit/checkpoint-1955/config.json\n",
            "Model weights saved in test-vit/checkpoint-1955/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1955/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2346\n",
            "Configuration saved in test-vit/checkpoint-2346/config.json\n",
            "Model weights saved in test-vit/checkpoint-2346/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2346/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2737\n",
            "Configuration saved in test-vit/checkpoint-2737/config.json\n",
            "Model weights saved in test-vit/checkpoint-2737/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2737/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3128\n",
            "Configuration saved in test-vit/checkpoint-3128/config.json\n",
            "Model weights saved in test-vit/checkpoint-3128/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3128/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3519\n",
            "Configuration saved in test-vit/checkpoint-3519/config.json\n",
            "Model weights saved in test-vit/checkpoint-3519/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3519/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3910\n",
            "Configuration saved in test-vit/checkpoint-3910/config.json\n",
            "Model weights saved in test-vit/checkpoint-3910/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3910/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-3910 (score: 0.3323967158794403).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3910, training_loss=0.43576326492192496, metrics={'train_runtime': 2030.8422, 'train_samples_per_second': 246.203, 'train_steps_per_second': 1.925, 'total_flos': 5.8828375719936e+19, 'train_loss': 0.43576326492192496, 'epoch': 10.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 16 patch, 6 attention heads, 2 layer decoder, 24 layers encoder\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I8LabM48x0Q_",
        "outputId": "b431f72c-1b39-4610-b955-eb08d3f1796b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 192\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2610\n",
            "  Number of trainable parameters = 22702464\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2610' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2610/2610 21:27, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.736200</td>\n",
              "      <td>0.597744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>0.489775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.472600</td>\n",
              "      <td>0.457702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.452800</td>\n",
              "      <td>0.441118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.434000</td>\n",
              "      <td>0.405080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.398900</td>\n",
              "      <td>0.378281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.383700</td>\n",
              "      <td>0.366899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.375800</td>\n",
              "      <td>0.361469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.371300</td>\n",
              "      <td>0.356990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.370100</td>\n",
              "      <td>0.357004</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-261\n",
            "Configuration saved in test-vit/checkpoint-261/config.json\n",
            "Model weights saved in test-vit/checkpoint-261/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-261/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-522\n",
            "Configuration saved in test-vit/checkpoint-522/config.json\n",
            "Model weights saved in test-vit/checkpoint-522/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-522/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-783\n",
            "Configuration saved in test-vit/checkpoint-783/config.json\n",
            "Model weights saved in test-vit/checkpoint-783/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-783/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1044\n",
            "Configuration saved in test-vit/checkpoint-1044/config.json\n",
            "Model weights saved in test-vit/checkpoint-1044/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1044/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1305\n",
            "Configuration saved in test-vit/checkpoint-1305/config.json\n",
            "Model weights saved in test-vit/checkpoint-1305/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1305/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1566\n",
            "Configuration saved in test-vit/checkpoint-1566/config.json\n",
            "Model weights saved in test-vit/checkpoint-1566/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1566/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1827\n",
            "Configuration saved in test-vit/checkpoint-1827/config.json\n",
            "Model weights saved in test-vit/checkpoint-1827/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1827/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2088\n",
            "Configuration saved in test-vit/checkpoint-2088/config.json\n",
            "Model weights saved in test-vit/checkpoint-2088/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2088/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2349\n",
            "Configuration saved in test-vit/checkpoint-2349/config.json\n",
            "Model weights saved in test-vit/checkpoint-2349/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2349/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2610\n",
            "Configuration saved in test-vit/checkpoint-2610/config.json\n",
            "Model weights saved in test-vit/checkpoint-2610/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2610/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-2349 (score: 0.35699009895324707).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2610, training_loss=0.45335129193419244, metrics={'train_runtime': 1302.4163, 'train_samples_per_second': 383.902, 'train_steps_per_second': 2.004, 'total_flos': 3.0569596452864e+19, 'train_loss': 0.45335129193419244, 'epoch': 10.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 16 patch, 6 attention heads, 2 layer decoder, no gradient accumulation\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IsLOmZCCppRw",
        "outputId": "e4b22097-dc01-4a21-cf8b-b89272db7af2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 96\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 2600\n",
            "  Number of trainable parameters = 25371648\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2213' max='2600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2213/2600 28:41 < 05:01, 1.28 it/s, Epoch 8.51/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.724100</td>\n",
              "      <td>0.570632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.497800</td>\n",
              "      <td>0.459864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.453600</td>\n",
              "      <td>0.441771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.422000</td>\n",
              "      <td>0.381374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.384100</td>\n",
              "      <td>0.362904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.341900</td>\n",
              "      <td>0.303518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.305000</td>\n",
              "      <td>0.269545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.284600</td>\n",
              "      <td>0.259065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-260\n",
            "Configuration saved in test-vit/checkpoint-260/config.json\n",
            "Model weights saved in test-vit/checkpoint-260/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-260/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-520\n",
            "Configuration saved in test-vit/checkpoint-520/config.json\n",
            "Model weights saved in test-vit/checkpoint-520/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-520/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-780\n",
            "Configuration saved in test-vit/checkpoint-780/config.json\n",
            "Model weights saved in test-vit/checkpoint-780/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-780/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1040\n",
            "Configuration saved in test-vit/checkpoint-1040/config.json\n",
            "Model weights saved in test-vit/checkpoint-1040/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1040/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1300\n",
            "Configuration saved in test-vit/checkpoint-1300/config.json\n",
            "Model weights saved in test-vit/checkpoint-1300/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1300/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1560\n",
            "Configuration saved in test-vit/checkpoint-1560/config.json\n",
            "Model weights saved in test-vit/checkpoint-1560/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1560/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1820\n",
            "Configuration saved in test-vit/checkpoint-1820/config.json\n",
            "Model weights saved in test-vit/checkpoint-1820/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1820/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2080\n",
            "Configuration saved in test-vit/checkpoint-2080/config.json\n",
            "Model weights saved in test-vit/checkpoint-2080/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2080/preprocessor_config.json\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d0eb842a1ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for two GPUs we want dataload_num_workers = 24 probably\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 16 patch, 6 attention heads, no gradient accumulation\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "Jx6GaR98dCSu",
        "outputId": "dbca765c-a867-4510-897f-b587f9ce7477"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 7820\n",
            "  Number of trainable parameters = 25371648\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1828' max='7820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1828/7820 09:03 < 29:44, 3.36 it/s, Epoch 2.34/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.689500</td>\n",
              "      <td>0.515039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.473000</td>\n",
              "      <td>0.448987</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-782\n",
            "Configuration saved in test-vit/checkpoint-782/config.json\n",
            "Model weights saved in test-vit/checkpoint-782/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-782/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1564\n",
            "Configuration saved in test-vit/checkpoint-1564/config.json\n",
            "Model weights saved in test-vit/checkpoint-1564/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1564/preprocessor_config.json\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d0eb842a1ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for two GPUs we want dataload_num_workers = 24 probably\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 16 patch, 6 attention heads, no gradient accumulation\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jlpTMfGrlnN2",
        "outputId": "ab32b93c-d0a7-426a-ec09-fcf953b0fbf7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 7810\n",
            "  Number of trainable parameters = 46332864\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3285' max='7810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3285/7810 2:40:30 < 3:41:13, 0.34 it/s, Epoch 4.20/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.643900</td>\n",
              "      <td>0.485830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.389600</td>\n",
              "      <td>0.324134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.319300</td>\n",
              "      <td>0.304766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.306000</td>\n",
              "      <td>0.295680</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-781\n",
            "Configuration saved in test-vit/checkpoint-781/config.json\n",
            "Model weights saved in test-vit/checkpoint-781/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-781/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1562\n",
            "Configuration saved in test-vit/checkpoint-1562/config.json\n",
            "Model weights saved in test-vit/checkpoint-1562/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1562/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2343\n",
            "Configuration saved in test-vit/checkpoint-2343/config.json\n",
            "Model weights saved in test-vit/checkpoint-2343/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2343/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3124\n",
            "Configuration saved in test-vit/checkpoint-3124/config.json\n",
            "Model weights saved in test-vit/checkpoint-3124/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3124/preprocessor_config.json\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2f04561c9425>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for two GPUs we want dataload_num_workers = 24 probably\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 8 patch, 6 attention heads, 24 layers\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jkhwJVRBX2LD",
        "outputId": "fff20ed9-65ed-481f-bcbd-b145d8b294a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3910\n",
            "  Number of trainable parameters = 11393280\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3910' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3910/3910 27:07, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.735500</td>\n",
              "      <td>0.593544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.521900</td>\n",
              "      <td>0.471758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.463000</td>\n",
              "      <td>0.450575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.447900</td>\n",
              "      <td>0.437822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.430100</td>\n",
              "      <td>0.394753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.392300</td>\n",
              "      <td>0.374182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.380500</td>\n",
              "      <td>0.364559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.372900</td>\n",
              "      <td>0.357252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.362600</td>\n",
              "      <td>0.342556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.356400</td>\n",
              "      <td>0.340533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-391\n",
            "Configuration saved in test-vit/checkpoint-391/config.json\n",
            "Model weights saved in test-vit/checkpoint-391/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-391/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-782\n",
            "Configuration saved in test-vit/checkpoint-782/config.json\n",
            "Model weights saved in test-vit/checkpoint-782/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-782/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1173\n",
            "Configuration saved in test-vit/checkpoint-1173/config.json\n",
            "Model weights saved in test-vit/checkpoint-1173/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1173/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1564\n",
            "Configuration saved in test-vit/checkpoint-1564/config.json\n",
            "Model weights saved in test-vit/checkpoint-1564/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1564/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1955\n",
            "Configuration saved in test-vit/checkpoint-1955/config.json\n",
            "Model weights saved in test-vit/checkpoint-1955/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1955/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2346\n",
            "Configuration saved in test-vit/checkpoint-2346/config.json\n",
            "Model weights saved in test-vit/checkpoint-2346/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2346/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2737\n",
            "Configuration saved in test-vit/checkpoint-2737/config.json\n",
            "Model weights saved in test-vit/checkpoint-2737/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2737/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3128\n",
            "Configuration saved in test-vit/checkpoint-3128/config.json\n",
            "Model weights saved in test-vit/checkpoint-3128/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3128/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3519\n",
            "Configuration saved in test-vit/checkpoint-3519/config.json\n",
            "Model weights saved in test-vit/checkpoint-3519/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3519/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3910\n",
            "Configuration saved in test-vit/checkpoint-3910/config.json\n",
            "Model weights saved in test-vit/checkpoint-3910/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3910/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-3910 (score: 0.34053346514701843).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3910, training_loss=0.44629948998961, metrics={'train_runtime': 1637.7505, 'train_samples_per_second': 305.297, 'train_steps_per_second': 2.387, 'total_flos': 1.5414111240192e+19, 'train_loss': 0.44629948998961, 'epoch': 10.0})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 16 patch, 4 attention heads NO gradient accumulation\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nLs4XxwyIFBl",
        "outputId": "8ecf5086-4b82-4fed-a9f6-43966a952569"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 7820\n",
            "  Number of trainable parameters = 25371648\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7820' max='7820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7820/7820 38:41, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.691800</td>\n",
              "      <td>0.520951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.476700</td>\n",
              "      <td>0.449636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.421800</td>\n",
              "      <td>0.381310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.377500</td>\n",
              "      <td>0.338242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.327900</td>\n",
              "      <td>0.288414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.295800</td>\n",
              "      <td>0.265134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.280100</td>\n",
              "      <td>0.251427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.271200</td>\n",
              "      <td>0.246482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.267100</td>\n",
              "      <td>0.242409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.265500</td>\n",
              "      <td>0.241822</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-782\n",
            "Configuration saved in test-vit/checkpoint-782/config.json\n",
            "Model weights saved in test-vit/checkpoint-782/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-782/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1564\n",
            "Configuration saved in test-vit/checkpoint-1564/config.json\n",
            "Model weights saved in test-vit/checkpoint-1564/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1564/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2346\n",
            "Configuration saved in test-vit/checkpoint-2346/config.json\n",
            "Model weights saved in test-vit/checkpoint-2346/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2346/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3128\n",
            "Configuration saved in test-vit/checkpoint-3128/config.json\n",
            "Model weights saved in test-vit/checkpoint-3128/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3128/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-3910\n",
            "Configuration saved in test-vit/checkpoint-3910/config.json\n",
            "Model weights saved in test-vit/checkpoint-3910/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-3910/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-4692\n",
            "Configuration saved in test-vit/checkpoint-4692/config.json\n",
            "Model weights saved in test-vit/checkpoint-4692/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-4692/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-5474\n",
            "Configuration saved in test-vit/checkpoint-5474/config.json\n",
            "Model weights saved in test-vit/checkpoint-5474/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-5474/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-6256\n",
            "Configuration saved in test-vit/checkpoint-6256/config.json\n",
            "Model weights saved in test-vit/checkpoint-6256/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-6256/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-7038\n",
            "Configuration saved in test-vit/checkpoint-7038/config.json\n",
            "Model weights saved in test-vit/checkpoint-7038/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-7038/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-7820\n",
            "Configuration saved in test-vit/checkpoint-7820/config.json\n",
            "Model weights saved in test-vit/checkpoint-7820/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-7820/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-7820 (score: 0.2418220192193985).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7820, training_loss=0.3675394511893582, metrics={'train_runtime': 2328.4553, 'train_samples_per_second': 214.735, 'train_steps_per_second': 3.358, 'total_flos': 3.4111881216e+19, 'train_loss': 0.3675394511893582, 'epoch': 10.0})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 16 patch, 6 attention heads NO gradient accumulation\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "efxgZVydvdrd",
        "outputId": "ba07b6da-5935-4637-c636-fea3b30075a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 192\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2610\n",
            "  Number of trainable parameters = 6474624\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='702' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 702/2610 05:38 < 15:22, 2.07 it/s, Epoch 2.69/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.773500</td>\n",
              "      <td>0.659215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.602900</td>\n",
              "      <td>0.548964</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-261\n",
            "Configuration saved in test-vit/checkpoint-261/config.json\n",
            "Model weights saved in test-vit/checkpoint-261/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-261/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-522\n",
            "Configuration saved in test-vit/checkpoint-522/config.json\n",
            "Model weights saved in test-vit/checkpoint-522/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-522/preprocessor_config.json\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-0f674700bbab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdecoder_intermediate_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m '''\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 16 patch, 3 attention heads NO gradient accumulation\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "\n",
        "'''\n",
        "configuration = ViTMAEConfig(\n",
        "    patch_size = 16,\n",
        "    image_size = 384,\n",
        "    mask_ratio = 0.6,\n",
        "    norm_pix_loss = True,\n",
        "    hidden_dropout_prob = 0.05,\n",
        "    attention_probs_dropout_prob = 0.05,\n",
        "    hidden_size = 256,\n",
        "    num_attention_heads = 4,\n",
        "    intermediate_size = 1024,\n",
        "    decoder_num_attention_heads = 4,\n",
        "    decoder_hidden_size = 128,\n",
        "    decoder_intermediate_size = 512\n",
        ")\n",
        "'''\n",
        "'''\n",
        "batch_size : int = 192\n",
        "'''\n",
        "'''\n",
        "decoder_num_attention_heads = 3,\n",
        "decoder_hidden_size = 96,\n",
        "decoder_intermediate_size = 384\n",
        "'''\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kFSIK6356_Lc",
        "outputId": "97c7df7b-7d0c-4d3a-c72c-f2e04e29ecfd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 192\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 3072\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 160\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 19:07, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.913700</td>\n",
              "      <td>0.813010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.785600</td>\n",
              "      <td>0.743591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.763500</td>\n",
              "      <td>0.766983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.792500</td>\n",
              "      <td>0.775378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.755700</td>\n",
              "      <td>0.727494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.731300</td>\n",
              "      <td>0.712803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.723500</td>\n",
              "      <td>0.710414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.721500</td>\n",
              "      <td>0.709038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.719700</td>\n",
              "      <td>0.707754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.705900</td>\n",
              "      <td>0.710080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-16\n",
            "Configuration saved in test-vit/checkpoint-16/config.json\n",
            "Model weights saved in test-vit/checkpoint-16/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-16/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-32\n",
            "Configuration saved in test-vit/checkpoint-32/config.json\n",
            "Model weights saved in test-vit/checkpoint-32/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-32/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-48\n",
            "Configuration saved in test-vit/checkpoint-48/config.json\n",
            "Model weights saved in test-vit/checkpoint-48/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-48/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-64\n",
            "Configuration saved in test-vit/checkpoint-64/config.json\n",
            "Model weights saved in test-vit/checkpoint-64/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-64/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-80\n",
            "Configuration saved in test-vit/checkpoint-80/config.json\n",
            "Model weights saved in test-vit/checkpoint-80/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-80/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-96\n",
            "Configuration saved in test-vit/checkpoint-96/config.json\n",
            "Model weights saved in test-vit/checkpoint-96/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-96/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-112\n",
            "Configuration saved in test-vit/checkpoint-112/config.json\n",
            "Model weights saved in test-vit/checkpoint-112/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-112/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-128\n",
            "Configuration saved in test-vit/checkpoint-128/config.json\n",
            "Model weights saved in test-vit/checkpoint-128/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-128/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-144\n",
            "Configuration saved in test-vit/checkpoint-144/config.json\n",
            "Model weights saved in test-vit/checkpoint-144/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-144/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-160\n",
            "Configuration saved in test-vit/checkpoint-160/config.json\n",
            "Model weights saved in test-vit/checkpoint-160/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-160/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-144 (score: 0.7077541351318359).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=160, training_loss=0.7612976551055908, metrics={'train_runtime': 1168.8065, 'train_samples_per_second': 427.787, 'train_steps_per_second': 0.137, 'total_flos': 8.798085340515533e+18, 'train_loss': 0.7612976551055908, 'epoch': 9.98})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 16 patch, 3 attention heads\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "\n",
        "'''\n",
        "configuration = ViTMAEConfig(\n",
        "    patch_size = 16,\n",
        "    image_size = 384,\n",
        "    mask_ratio = 0.6,\n",
        "    norm_pix_loss = True,\n",
        "    hidden_dropout_prob = 0.05,\n",
        "    attention_probs_dropout_prob = 0.05,\n",
        "    hidden_size = 256,\n",
        "    num_attention_heads = 4,\n",
        "    intermediate_size = 1024,\n",
        "    decoder_num_attention_heads = 4,\n",
        "    decoder_hidden_size = 128,\n",
        "    decoder_intermediate_size = 512\n",
        ")\n",
        "'''\n",
        "'''\n",
        "batch_size : int = 192\n",
        "'''\n",
        "'''\n",
        "decoder_num_attention_heads = 3,\n",
        "decoder_hidden_size = 96,\n",
        "decoder_intermediate_size = 384\n",
        "'''\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "19hk0y4Tz4Ts",
        "outputId": "a1ebf521-9c2e-4aa5-ece5-46c3dadc1133"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 192\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 3072\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 160\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 86/160 10:44 < 09:28, 0.13 it/s, Epoch 5.31/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.909200</td>\n",
              "      <td>0.790385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.782100</td>\n",
              "      <td>0.753281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.760300</td>\n",
              "      <td>0.737416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.746200</td>\n",
              "      <td>0.724553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.733300</td>\n",
              "      <td>0.719171</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-16\n",
            "Configuration saved in test-vit/checkpoint-16/config.json\n",
            "Model weights saved in test-vit/checkpoint-16/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-16/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-32\n",
            "Configuration saved in test-vit/checkpoint-32/config.json\n",
            "Model weights saved in test-vit/checkpoint-32/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-32/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-48\n",
            "Configuration saved in test-vit/checkpoint-48/config.json\n",
            "Model weights saved in test-vit/checkpoint-48/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-48/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-64\n",
            "Configuration saved in test-vit/checkpoint-64/config.json\n",
            "Model weights saved in test-vit/checkpoint-64/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-64/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-80\n",
            "Configuration saved in test-vit/checkpoint-80/config.json\n",
            "Model weights saved in test-vit/checkpoint-80/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-80/preprocessor_config.json\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-fb1be8aea8eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdecoder_intermediate_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m '''\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1502\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1504\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1505\u001b[0m         )\n\u001b[1;32m   1506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2502\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2504\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 16 patch\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "\n",
        "'''\n",
        "configuration = ViTMAEConfig(\n",
        "    patch_size = 16,\n",
        "    image_size = 384,\n",
        "    mask_ratio = 0.6,\n",
        "    norm_pix_loss = True,\n",
        "    hidden_dropout_prob = 0.05,\n",
        "    attention_probs_dropout_prob = 0.05,\n",
        "    hidden_size = 256,\n",
        "    num_attention_heads = 4,\n",
        "    intermediate_size = 1024,\n",
        "    decoder_num_attention_heads = 4,\n",
        "    decoder_hidden_size = 128,\n",
        "    decoder_intermediate_size = 512\n",
        ")\n",
        "'''\n",
        "'''\n",
        "batch_size : int = 192\n",
        "'''\n",
        "'''\n",
        "decoder_num_attention_heads = 3,\n",
        "decoder_hidden_size = 96,\n",
        "decoder_intermediate_size = 384\n",
        "'''\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bOWVpXpXs69k",
        "outputId": "dcfa454a-bfb6-4eb2-bd09-6e49e0d97e17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 192\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 3072\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 160\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 20:14, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.882000</td>\n",
              "      <td>0.796675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.804900</td>\n",
              "      <td>0.734464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.743800</td>\n",
              "      <td>0.746208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.833700</td>\n",
              "      <td>0.822157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.858300</td>\n",
              "      <td>0.863542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.877200</td>\n",
              "      <td>0.850637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.864800</td>\n",
              "      <td>0.830015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.822100</td>\n",
              "      <td>0.774627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.790400</td>\n",
              "      <td>0.764449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.761500</td>\n",
              "      <td>0.761620</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-16\n",
            "Configuration saved in test-vit/checkpoint-16/config.json\n",
            "Model weights saved in test-vit/checkpoint-16/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-16/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-32\n",
            "Configuration saved in test-vit/checkpoint-32/config.json\n",
            "Model weights saved in test-vit/checkpoint-32/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-32/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-48\n",
            "Configuration saved in test-vit/checkpoint-48/config.json\n",
            "Model weights saved in test-vit/checkpoint-48/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-48/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-64\n",
            "Configuration saved in test-vit/checkpoint-64/config.json\n",
            "Model weights saved in test-vit/checkpoint-64/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-64/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-80\n",
            "Configuration saved in test-vit/checkpoint-80/config.json\n",
            "Model weights saved in test-vit/checkpoint-80/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-80/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-96\n",
            "Configuration saved in test-vit/checkpoint-96/config.json\n",
            "Model weights saved in test-vit/checkpoint-96/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-96/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-112\n",
            "Configuration saved in test-vit/checkpoint-112/config.json\n",
            "Model weights saved in test-vit/checkpoint-112/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-112/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-128\n",
            "Configuration saved in test-vit/checkpoint-128/config.json\n",
            "Model weights saved in test-vit/checkpoint-128/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-128/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-144\n",
            "Configuration saved in test-vit/checkpoint-144/config.json\n",
            "Model weights saved in test-vit/checkpoint-144/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-144/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-160\n",
            "Configuration saved in test-vit/checkpoint-160/config.json\n",
            "Model weights saved in test-vit/checkpoint-160/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-160/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-32 (score: 0.7344642877578735).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=160, training_loss=0.8238676428794861, metrics={'train_runtime': 1235.566, 'train_samples_per_second': 404.673, 'train_steps_per_second': 0.129, 'total_flos': 1.4403846729432564e+19, 'train_loss': 0.8238676428794861, 'epoch': 9.98})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 16 patch\n",
        "# random perspective augmentation + random adjust sharpness 0.38 sharpess factor\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "\n",
        "'''\n",
        "configuration = ViTMAEConfig(\n",
        "    patch_size = 16,\n",
        "    image_size = 384,\n",
        "    mask_ratio = 0.6,\n",
        "    norm_pix_loss = True,\n",
        "    hidden_dropout_prob = 0.05,\n",
        "    attention_probs_dropout_prob = 0.05,\n",
        "    hidden_size = 256,\n",
        "    num_attention_heads = 4,\n",
        "    intermediate_size = 1024,\n",
        "    decoder_num_attention_heads = 4,\n",
        "    decoder_hidden_size = 128,\n",
        "    decoder_intermediate_size = 512\n",
        ")\n",
        "'''\n",
        "'''\n",
        "batch_size : int = 192\n",
        "'''\n",
        "'''\n",
        "decoder_num_attention_heads = 3,\n",
        "decoder_hidden_size = 96,\n",
        "decoder_intermediate_size = 384\n",
        "'''\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_AKMYk5kp054",
        "outputId": "5407534c-3454-4482-b07d-4419c577b290"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 192\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 3072\n",
            "  Gradient Accumulation steps = 16\n",
            "  Total optimization steps = 160\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 20:06, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.880100</td>\n",
              "      <td>0.769967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.756800</td>\n",
              "      <td>0.734979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.761700</td>\n",
              "      <td>0.774884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.796200</td>\n",
              "      <td>0.780854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.786900</td>\n",
              "      <td>0.827051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.780500</td>\n",
              "      <td>0.711354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.722900</td>\n",
              "      <td>0.707125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.718700</td>\n",
              "      <td>0.709463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.717100</td>\n",
              "      <td>0.703949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.704100</td>\n",
              "      <td>0.707775</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-16\n",
            "Configuration saved in test-vit/checkpoint-16/config.json\n",
            "Model weights saved in test-vit/checkpoint-16/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-16/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-32\n",
            "Configuration saved in test-vit/checkpoint-32/config.json\n",
            "Model weights saved in test-vit/checkpoint-32/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-32/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-48\n",
            "Configuration saved in test-vit/checkpoint-48/config.json\n",
            "Model weights saved in test-vit/checkpoint-48/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-48/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-64\n",
            "Configuration saved in test-vit/checkpoint-64/config.json\n",
            "Model weights saved in test-vit/checkpoint-64/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-64/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-80\n",
            "Configuration saved in test-vit/checkpoint-80/config.json\n",
            "Model weights saved in test-vit/checkpoint-80/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-80/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-96\n",
            "Configuration saved in test-vit/checkpoint-96/config.json\n",
            "Model weights saved in test-vit/checkpoint-96/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-96/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-112\n",
            "Configuration saved in test-vit/checkpoint-112/config.json\n",
            "Model weights saved in test-vit/checkpoint-112/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-112/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-128\n",
            "Configuration saved in test-vit/checkpoint-128/config.json\n",
            "Model weights saved in test-vit/checkpoint-128/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-128/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-144\n",
            "Configuration saved in test-vit/checkpoint-144/config.json\n",
            "Model weights saved in test-vit/checkpoint-144/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-144/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-160\n",
            "Configuration saved in test-vit/checkpoint-160/config.json\n",
            "Model weights saved in test-vit/checkpoint-160/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-160/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-144 (score: 0.7039489150047302).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=160, training_loss=0.7625102281570435, metrics={'train_runtime': 1226.502, 'train_samples_per_second': 407.663, 'train_steps_per_second': 0.13, 'total_flos': 1.4403846729432564e+19, 'train_loss': 0.7625102281570435, 'epoch': 9.98})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 16 patch\n",
        "# only random perspective augmentation\n",
        "# 60% masking ratio + hyperparams in original implementation like cosine learning rate scheduling\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "\n",
        "'''\n",
        "configuration = ViTMAEConfig(\n",
        "    patch_size = 16,\n",
        "    image_size = 384,\n",
        "    mask_ratio = 0.6,\n",
        "    norm_pix_loss = True,\n",
        "    hidden_dropout_prob = 0.05,\n",
        "    attention_probs_dropout_prob = 0.05,\n",
        "    hidden_size = 256,\n",
        "    num_attention_heads = 4,\n",
        "    intermediate_size = 1024,\n",
        "    decoder_num_attention_heads = 4,\n",
        "    decoder_hidden_size = 128,\n",
        "    decoder_intermediate_size = 512\n",
        ")\n",
        "'''\n",
        "'''\n",
        "batch_size : int = 192\n",
        "'''\n",
        "'''\n",
        "decoder_num_attention_heads = 3,\n",
        "decoder_hidden_size = 96,\n",
        "decoder_intermediate_size = 384\n",
        "'''\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HSz-AewWIKl-",
        "outputId": "75f7e26f-7990-4804-a815-1e0737b9c670"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 192\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2610\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2610' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2610/2610 23:42, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.667600</td>\n",
              "      <td>0.617691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.494500</td>\n",
              "      <td>0.442654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.419800</td>\n",
              "      <td>0.385528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.373100</td>\n",
              "      <td>0.354898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.345900</td>\n",
              "      <td>0.324455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.313900</td>\n",
              "      <td>0.287452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.288600</td>\n",
              "      <td>0.271211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.276300</td>\n",
              "      <td>0.259600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.267900</td>\n",
              "      <td>0.250975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.262200</td>\n",
              "      <td>0.247330</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-261\n",
            "Configuration saved in test-vit/checkpoint-261/config.json\n",
            "Model weights saved in test-vit/checkpoint-261/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-261/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-522\n",
            "Configuration saved in test-vit/checkpoint-522/config.json\n",
            "Model weights saved in test-vit/checkpoint-522/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-522/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-783\n",
            "Configuration saved in test-vit/checkpoint-783/config.json\n",
            "Model weights saved in test-vit/checkpoint-783/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-783/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1044\n",
            "Configuration saved in test-vit/checkpoint-1044/config.json\n",
            "Model weights saved in test-vit/checkpoint-1044/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1044/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1305\n",
            "Configuration saved in test-vit/checkpoint-1305/config.json\n",
            "Model weights saved in test-vit/checkpoint-1305/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1305/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1566\n",
            "Configuration saved in test-vit/checkpoint-1566/config.json\n",
            "Model weights saved in test-vit/checkpoint-1566/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1566/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1827\n",
            "Configuration saved in test-vit/checkpoint-1827/config.json\n",
            "Model weights saved in test-vit/checkpoint-1827/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1827/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2088\n",
            "Configuration saved in test-vit/checkpoint-2088/config.json\n",
            "Model weights saved in test-vit/checkpoint-2088/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2088/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2349\n",
            "Configuration saved in test-vit/checkpoint-2349/config.json\n",
            "Model weights saved in test-vit/checkpoint-2349/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2349/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2610\n",
            "Configuration saved in test-vit/checkpoint-2610/config.json\n",
            "Model weights saved in test-vit/checkpoint-2610/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2610/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-2610 (score: 0.2473297119140625).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2610, training_loss=0.3709715715313323, metrics={'train_runtime': 1441.5912, 'train_samples_per_second': 346.839, 'train_steps_per_second': 1.81, 'total_flos': 1.5414111234152202e+19, 'train_loss': 0.3709715715313323, 'epoch': 10.0})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 60% masking ratio\n",
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 4 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "\n",
        "'''\n",
        "configuration = ViTMAEConfig(\n",
        "    patch_size = 16,\n",
        "    image_size = 384,\n",
        "    mask_ratio = 0.6,\n",
        "    norm_pix_loss = True,\n",
        "    hidden_dropout_prob = 0.05,\n",
        "    attention_probs_dropout_prob = 0.05,\n",
        "    hidden_size = 256,\n",
        "    num_attention_heads = 4,\n",
        "    intermediate_size = 1024,\n",
        "    decoder_num_attention_heads = 4,\n",
        "    decoder_hidden_size = 128,\n",
        "    decoder_intermediate_size = 512\n",
        ")\n",
        "'''\n",
        "'''\n",
        "batch_size : int = 192\n",
        "'''\n",
        "'''\n",
        "decoder_num_attention_heads = 3,\n",
        "decoder_hidden_size = 96,\n",
        "decoder_intermediate_size = 384\n",
        "'''\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M4MA7jPsyDED",
        "outputId": "dbaea526-c332-45a3-f63e-2ebb16ab2f31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 192\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2610\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2610' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2610/2610 22:26, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.662800</td>\n",
              "      <td>0.593521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.460900</td>\n",
              "      <td>0.405403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.370800</td>\n",
              "      <td>0.340977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.319300</td>\n",
              "      <td>0.301016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.304300</td>\n",
              "      <td>0.288528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.281200</td>\n",
              "      <td>0.251449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.247000</td>\n",
              "      <td>0.218862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.223300</td>\n",
              "      <td>0.195648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.204500</td>\n",
              "      <td>0.180604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.194700</td>\n",
              "      <td>0.175687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-261\n",
            "Configuration saved in test-vit/checkpoint-261/config.json\n",
            "Model weights saved in test-vit/checkpoint-261/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-261/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-522\n",
            "Configuration saved in test-vit/checkpoint-522/config.json\n",
            "Model weights saved in test-vit/checkpoint-522/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-522/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-783\n",
            "Configuration saved in test-vit/checkpoint-783/config.json\n",
            "Model weights saved in test-vit/checkpoint-783/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-783/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1044\n",
            "Configuration saved in test-vit/checkpoint-1044/config.json\n",
            "Model weights saved in test-vit/checkpoint-1044/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1044/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1305\n",
            "Configuration saved in test-vit/checkpoint-1305/config.json\n",
            "Model weights saved in test-vit/checkpoint-1305/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1305/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1566\n",
            "Configuration saved in test-vit/checkpoint-1566/config.json\n",
            "Model weights saved in test-vit/checkpoint-1566/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1566/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-1827\n",
            "Configuration saved in test-vit/checkpoint-1827/config.json\n",
            "Model weights saved in test-vit/checkpoint-1827/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-1827/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2088\n",
            "Configuration saved in test-vit/checkpoint-2088/config.json\n",
            "Model weights saved in test-vit/checkpoint-2088/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2088/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2349\n",
            "Configuration saved in test-vit/checkpoint-2349/config.json\n",
            "Model weights saved in test-vit/checkpoint-2349/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2349/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-2610\n",
            "Configuration saved in test-vit/checkpoint-2610/config.json\n",
            "Model weights saved in test-vit/checkpoint-2610/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-2610/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-2610 (score: 0.17568740248680115).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2610, training_loss=0.3268731566681259, metrics={'train_runtime': 1362.5559, 'train_samples_per_second': 366.957, 'train_steps_per_second': 1.916, 'total_flos': 8.813032373223424e+18, 'train_loss': 0.3268731566681259, 'epoch': 10.0})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dataload_num_workers = 12 - does it beat 1.76 it/s? -> YES IT DOES!\n",
        "# for two GPUs we want dataload_num_workers = 24 probably\n",
        "# 3 attention heads encoder, even smaller decoder with 16 patch size can do:\n",
        "\n",
        "'''\n",
        "configuration = ViTMAEConfig(\n",
        "    patch_size = 16,\n",
        "    image_size = 384,\n",
        "    mask_ratio = 0.6,\n",
        "    norm_pix_loss = True,\n",
        "    hidden_dropout_prob = 0.05,\n",
        "    attention_probs_dropout_prob = 0.05,\n",
        "    hidden_size = 192,\n",
        "    num_attention_heads = 3,\n",
        "    intermediate_size = 768,\n",
        "    decoder_num_attention_heads = 3,\n",
        "    decoder_hidden_size = 96,\n",
        "    decoder_intermediate_size = 384\n",
        ")\n",
        "'''\n",
        "'''\n",
        "batch_size : int = 192\n",
        "'''\n",
        "'''\n",
        "decoder_num_attention_heads = 3,\n",
        "decoder_hidden_size = 96,\n",
        "decoder_intermediate_size = 384\n",
        "'''\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1OZChBUUgidx",
        "outputId": "6e7477ce-c74d-4bd0-a960-fdda485f7376"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 50000\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8192\n",
            "  Gradient Accumulation steps = 128\n",
            "  Total optimization steps = 30\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 12:34, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.904200</td>\n",
              "      <td>23.007744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>7.925800</td>\n",
              "      <td>1.005459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.089300</td>\n",
              "      <td>0.935803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.896900</td>\n",
              "      <td>0.837603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.822400</td>\n",
              "      <td>0.804329</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-6\n",
            "Configuration saved in test-vit/checkpoint-6/config.json\n",
            "Model weights saved in test-vit/checkpoint-6/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-6/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-12\n",
            "Configuration saved in test-vit/checkpoint-12/config.json\n",
            "Model weights saved in test-vit/checkpoint-12/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-12/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-18\n",
            "Configuration saved in test-vit/checkpoint-18/config.json\n",
            "Model weights saved in test-vit/checkpoint-18/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-18/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-24\n",
            "Configuration saved in test-vit/checkpoint-24/config.json\n",
            "Model weights saved in test-vit/checkpoint-24/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-24/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4096\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to test-vit/checkpoint-30\n",
            "Configuration saved in test-vit/checkpoint-30/config.json\n",
            "Model weights saved in test-vit/checkpoint-30/pytorch_model.bin\n",
            "Feature extractor saved in test-vit/checkpoint-30/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from test-vit/checkpoint-30 (score: 0.8043293952941895).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=30, training_loss=3.1277314186096192, metrics={'train_runtime': 781.5227, 'train_samples_per_second': 319.888, 'train_steps_per_second': 0.038, 'total_flos': 6.25123267779022e+18, 'train_loss': 3.1277314186096192, 'epoch': 4.98})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3 attention heads encoder, 3 attention heads decoder\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk0fI9uqdxyd"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f9851d550284898a6ac1e6342bb9f09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c032fa22bf4604980128d4a5fdac78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ab5555c14fa483b9a5274afa9fed01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bb3c13d9794423683f780737ac52e21",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91a25778711b40c2a9d007929e5e3461",
            "value": 2
          }
        },
        "51abf02cb87d4229a36c01d6866998ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a0c7c5241d745e2b5304798c5c0f4a1",
              "IPY_MODEL_4ab5555c14fa483b9a5274afa9fed01a",
              "IPY_MODEL_721d602c6b694387bd1d04bd7ac02ebe"
            ],
            "layout": "IPY_MODEL_2f9851d550284898a6ac1e6342bb9f09"
          }
        },
        "56494e8b465f4893ad9d13346df2853f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a0c7c5241d745e2b5304798c5c0f4a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31c032fa22bf4604980128d4a5fdac78",
            "placeholder": "​",
            "style": "IPY_MODEL_a204391cc76a4a14a185574912240f26",
            "value": "100%"
          }
        },
        "721d602c6b694387bd1d04bd7ac02ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cffbe1fbc48e4df78a44413f20b5b78b",
            "placeholder": "​",
            "style": "IPY_MODEL_56494e8b465f4893ad9d13346df2853f",
            "value": " 2/2 [00:00&lt;00:00, 57.78it/s]"
          }
        },
        "91a25778711b40c2a9d007929e5e3461": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bb3c13d9794423683f780737ac52e21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a204391cc76a4a14a185574912240f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cffbe1fbc48e4df78a44413f20b5b78b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
