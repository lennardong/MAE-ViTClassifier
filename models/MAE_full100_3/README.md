---
tags:
- masked-auto-encoding
- generated_from_trainer
datasets:
- imagefolder
model-index:
- name: MAE_full100_3
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# MAE_full100_3

This model is a fine-tuned version of [](https://huggingface.co/) on the /home/NUS-NeuralNetworks dataset.
It achieves the following results on the evaluation set:
- Loss: 0.5655

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.000125
- train_batch_size: 160
- eval_batch_size: 60
- seed: 12
- gradient_accumulation_steps: 4
- total_train_batch_size: 640
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 100.0

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 0.984         | 0.98  | 31   | 0.8004          |
| 0.7377        | 2.0   | 63   | 0.7255          |
| 0.7363        | 2.98  | 94   | 0.7246          |
| 0.7125        | 4.0   | 126  | 0.7216          |
| 0.7289        | 4.98  | 157  | 0.7080          |
| 0.691         | 6.0   | 189  | 0.7021          |
| 0.7059        | 6.98  | 220  | 0.6958          |
| 0.6792        | 8.0   | 252  | 0.6892          |
| 0.6982        | 8.98  | 283  | 0.6878          |
| 0.6759        | 10.0  | 315  | 0.6864          |
| 0.6971        | 10.98 | 346  | 0.6871          |
| 0.6754        | 12.0  | 378  | 0.6868          |
| 0.6967        | 12.98 | 409  | 0.6861          |
| 0.6744        | 14.0  | 441  | 0.6860          |
| 0.6952        | 14.98 | 472  | 0.6842          |
| 0.6718        | 16.0  | 504  | 0.6817          |
| 0.6924        | 16.98 | 535  | 0.6796          |
| 0.6701        | 18.0  | 567  | 0.6806          |
| 0.6904        | 18.98 | 598  | 0.6806          |
| 0.6688        | 20.0  | 630  | 0.6772          |
| 0.6893        | 20.98 | 661  | 0.6763          |
| 0.6668        | 22.0  | 693  | 0.6764          |
| 0.6874        | 22.98 | 724  | 0.6760          |
| 0.6659        | 24.0  | 756  | 0.6765          |
| 0.6869        | 24.98 | 787  | 0.6760          |
| 0.666         | 26.0  | 819  | 0.6763          |
| 0.6852        | 26.98 | 850  | 0.6772          |
| 0.6632        | 28.0  | 882  | 0.6744          |
| 0.685         | 28.98 | 913  | 0.6737          |
| 0.6621        | 30.0  | 945  | 0.6723          |
| 0.6795        | 30.98 | 976  | 0.6670          |
| 0.6583        | 32.0  | 1008 | 0.6670          |
| 0.6768        | 32.98 | 1039 | 0.6658          |
| 0.6548        | 34.0  | 1071 | 0.6647          |
| 0.6745        | 34.98 | 1102 | 0.6610          |
| 0.6511        | 36.0  | 1134 | 0.6624          |
| 0.6716        | 36.98 | 1165 | 0.6602          |
| 0.6487        | 38.0  | 1197 | 0.6601          |
| 0.6685        | 38.98 | 1228 | 0.6556          |
| 0.6467        | 40.0  | 1260 | 0.6581          |
| 0.6647        | 40.98 | 1291 | 0.6525          |
| 0.6395        | 42.0  | 1323 | 0.6467          |
| 0.6561        | 42.98 | 1354 | 0.6419          |
| 0.6318        | 44.0  | 1386 | 0.6405          |
| 0.6496        | 44.98 | 1417 | 0.6388          |
| 0.6283        | 46.0  | 1449 | 0.6414          |
| 0.6455        | 46.98 | 1480 | 0.6322          |
| 0.6202        | 48.0  | 1512 | 0.6290          |
| 0.6369        | 48.98 | 1543 | 0.6248          |
| 0.6151        | 50.0  | 1575 | 0.6254          |
| 0.63          | 50.98 | 1606 | 0.6161          |
| 0.607         | 52.0  | 1638 | 0.6156          |
| 0.6211        | 52.98 | 1669 | 0.6100          |
| 0.601         | 54.0  | 1701 | 0.6083          |
| 0.6155        | 54.98 | 1732 | 0.6043          |
| 0.5923        | 56.0  | 1764 | 0.5993          |
| 0.6092        | 56.98 | 1795 | 0.5982          |
| 0.5881        | 58.0  | 1827 | 0.5975          |
| 0.6051        | 58.98 | 1858 | 0.5941          |
| 0.5822        | 60.0  | 1890 | 0.5954          |
| 0.6           | 60.98 | 1921 | 0.5894          |
| 0.5777        | 62.0  | 1953 | 0.5856          |
| 0.5954        | 62.98 | 1984 | 0.5840          |
| 0.5742        | 64.0  | 2016 | 0.5849          |
| 0.5922        | 64.98 | 2047 | 0.5804          |
| 0.5715        | 66.0  | 2079 | 0.5789          |
| 0.5907        | 66.98 | 2110 | 0.5799          |
| 0.5689        | 68.0  | 2142 | 0.5766          |
| 0.5853        | 68.98 | 2173 | 0.5772          |
| 0.5661        | 70.0  | 2205 | 0.5750          |
| 0.5829        | 70.98 | 2236 | 0.5734          |
| 0.564         | 72.0  | 2268 | 0.5719          |
| 0.5807        | 72.98 | 2299 | 0.5721          |
| 0.562         | 74.0  | 2331 | 0.5709          |
| 0.5794        | 74.98 | 2362 | 0.5695          |
| 0.5604        | 76.0  | 2394 | 0.5710          |
| 0.5784        | 76.98 | 2425 | 0.5694          |
| 0.5597        | 78.0  | 2457 | 0.5696          |
| 0.577         | 78.98 | 2488 | 0.5683          |
| 0.5586        | 80.0  | 2520 | 0.5684          |
| 0.5761        | 80.98 | 2551 | 0.5678          |
| 0.5574        | 82.0  | 2583 | 0.5670          |
| 0.5757        | 82.98 | 2614 | 0.5666          |
| 0.5567        | 84.0  | 2646 | 0.5666          |
| 0.5749        | 84.98 | 2677 | 0.5665          |
| 0.5569        | 86.0  | 2709 | 0.5666          |
| 0.575         | 86.98 | 2740 | 0.5670          |
| 0.5566        | 88.0  | 2772 | 0.5660          |
| 0.5745        | 88.98 | 2803 | 0.5648          |
| 0.5558        | 90.0  | 2835 | 0.5652          |
| 0.5739        | 90.98 | 2866 | 0.5650          |
| 0.5547        | 92.0  | 2898 | 0.5660          |
| 0.574         | 92.98 | 2929 | 0.5648          |
| 0.5559        | 94.0  | 2961 | 0.5652          |
| 0.5732        | 94.98 | 2992 | 0.5649          |
| 0.5554        | 96.0  | 3024 | 0.5648          |
| 0.5735        | 96.98 | 3055 | 0.5651          |
| 0.5562        | 98.0  | 3087 | 0.5650          |
| 0.5645        | 98.41 | 3100 | 0.5652          |


### Framework versions

- Transformers 4.35.0.dev0
- Pytorch 2.0.0
- Datasets 2.12.0
- Tokenizers 0.14.1
